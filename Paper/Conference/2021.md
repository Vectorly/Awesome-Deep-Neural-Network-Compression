# ICLR

## Pruning
- A Gradient Flow Framework for Analyzing Network Pruning


## Quantization
- Degree-quant: Quantization-aware Training for Graph Neural Networks
- Training With Quantization Noise for Extreme Model Compression
- Brecq: Pushing the Limit of Post-training Quantization by Block Reconstruction
- Neural Gradients Are Near-lognormal: Improved Quantized and Sparse Training
- Reducing the Computational Cost of Deep Generative Models with Binary Neural Networks
- Bipointnet: Binary Neural Network for Point Clouds
- Faster Binary Embeddings for Preserving Euclidean Distances
- Growing Efficient Deep Networks by Structured Continuous Sparsification
- CPT: Efficient Deep Neural Network Training Via Cyclic Precision

## Distillation
- Mixkd: Towards Efficient Distillation of Large-scale Language Models
- Knowledge Distillation As Semiparametric Inference
- A Teacher-student Framework to Distill Future Trajectories
- Is Label Smoothing Truly Incompatible with Knowledge Distillation: an Empirical Study
- Rethinking Soft Labels for Knowledge Distillation: a Bias-variance Tradeoff Perspective
- Neural Attention Distillation: Erasing Back-door Triggers from Deep Neural Networks
- Knowledge Distillation Via Softmax Regres-sion Representation Learning